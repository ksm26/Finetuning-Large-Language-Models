{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa404373",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b2974",
   "metadata": {},
   "source": [
    "### Technically, there are very few steps to run it on GPUs, elsewhere (ie. on Lamini).\n",
    "```\n",
    "finetuned_model = BasicModelRunner(\n",
    "    \"lamini/lamini_docs_finetuned\"\n",
    ")\n",
    "finetuned_output = finetuned_model(\n",
    "    test_dataset_list # batched!\n",
    ") \n",
    "```\n",
    "\n",
    "### Let's look again under the hood! This is the open core code of Lamini's `llama` library :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fa1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"lamini/lamini_docs\")\n",
    "\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset[0][\"question\"])\n",
    "print(test_dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ef08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lamini/lamini_docs_finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101f82b",
   "metadata": {},
   "source": [
    "## Setup a really basic evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_exact_match(a, b):\n",
    "    return a.strip() == b.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04747b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  input_ids = tokenizer.encode(\n",
    "      text,\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06fb78b",
   "metadata": {},
   "source": [
    "## Run model and compare to expected answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = test_dataset[0][\"question\"]\n",
    "generated_answer = inference(test_question, model, tokenizer)\n",
    "print(test_question)\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5aba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = test_dataset[0][\"answer\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = is_exact_match(generated_answer, answer)\n",
    "print(exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a623146",
   "metadata": {},
   "source": [
    "## Run over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "metrics = {'exact_matches': []}\n",
    "predictions = []\n",
    "for i, item in tqdm(enumerate(test_dataset)):\n",
    "    print(\"i Evaluating: \" + str(item))\n",
    "    question = item['question']\n",
    "    answer = item['answer']\n",
    "\n",
    "    try:\n",
    "      predicted_answer = inference(question, model, tokenizer)\n",
    "    except:\n",
    "      continue\n",
    "    predictions.append([predicted_answer, answer])\n",
    "\n",
    "    #fixed: exact_match = is_exact_match(generated_answer, answer)\n",
    "    exact_match = is_exact_match(predicted_answer, answer)\n",
    "    metrics['exact_matches'].append(exact_match)\n",
    "\n",
    "    if i > n and n != -1:\n",
    "      break\n",
    "print('Number of exact matches: ', sum(metrics['exact_matches']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions, columns=[\"predicted_answer\", \"target_answer\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0141bd04",
   "metadata": {},
   "source": [
    "## Evaluate all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset_path = \"lamini/lamini_docs_evaluation\"\n",
    "evaluation_dataset = datasets.load_dataset(evaluation_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fb53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d0a8b",
   "metadata": {},
   "source": [
    "## Try the ARC benchmark\n",
    "This can take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python lm-evaluation-harness/main.py --model hf-causal --model_args pretrained=lamini/lamini_docs_finetuned --tasks arc_easy --device cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
